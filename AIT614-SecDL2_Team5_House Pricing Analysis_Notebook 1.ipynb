{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8933c514-498e-4490-b706-aa89a9cf41da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### AIT 614 - Big Data Essentials <br>\n",
    "#### Final Project: House Pricing Analysis\n",
    "#### Notebook 1\n",
    "\n",
    "Course Section #: AIT 614-DL2<br>\n",
    "Team 5: Nafisa Ahmed, Peter Bishay, Charles Gilbertson, Sneha Kumaran, Cora Sula<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b462ec1-7d84-4169-9e80-447c2586f993",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Data Cleansing/Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88312785-13bb-46be-b7ad-ff6030197757",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import dataset\n",
    "housing_data = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/nahmed27@gmu.edu/DC_Properties.csv\", inferSchema = \"true\") #added inferschema to produce the ML algorithm\n",
    "\n",
    "# display the first few rows of the dataset\n",
    "display(housing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60656906-2351-408d-a30d-fe3100b14334",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the total rows of the dataset\n",
    "housing_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59c49d01-d334-46c9-b9c9-ac1b3b5d6071",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display summary/descriptive statistics\n",
    "display(housing_data.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25773063-bce0-4b73-b3e2-bf45cd596424",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display the data schema \n",
    "housing_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b102bd4-72fb-439a-ab2e-30e4f1f494dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "column_to_drop = [\"_c0\",\"SALE_NUM\", \"FULLADDRESS\",\"CITY\",\"STATE\",\"ZIPCODE\", \"NATIONALGRID\",\"LATITUDE\",\"LONGITUDE\",\"ASSESSMENT_NBHD\", \"ASSESSMENT_SUBNBHD\", \"CENSUS_TRACT\", \"CENSUS_BLOCK\", \"SQUARE\", \"X\", \"Y\", \"CMPLX_NUM\", \"USECODE\", \"GIS_LAST_MOD_DTTM\"]            \n",
    "\n",
    "housing_data = housing_data.drop(*column_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68b03c3e-0ae4-4866-835f-508c9b7d511d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check for NULL Values\n",
    "from pyspark.sql.functions import col, count, when\n",
    "null_counts = housing_data.select([count(when(col(c).isNull(), c)).alias(c) for c in housing_data.columns])\n",
    "\n",
    "# Display the null value counts\n",
    "display(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21e4294e-0000-475a-8369-ab0a4a982ec2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# drop the NULL values for the 'PRICE' variable\n",
    "housing_data = housing_data.dropna(subset=[\"PRICE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "927bbe2f-0bca-4297-b522-a2df1d1115c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display total rows for the data\n",
    "housing_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7653f32-bf1c-42f6-a7f6-d9fe1b06c97f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check for NULL Values again\n",
    "null_counts = housing_data.select([count(when(col(c).isNull(), c)).alias(c) for c in housing_data.columns])\n",
    "\n",
    "# Display the null counts row\n",
    "display(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "142454f4-3505-42e2-8d62-6cb3b7b32b06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display the data schema \n",
    "housing_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e621ec45-bc75-4960-9ed4-b4145120f0b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display the first few rows of the dataset\n",
    "display(housing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af81f0e9-0f8b-4af8-9f8c-a745fd8ffaa9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76aac503-8141-4a57-8305-3097cb86648e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a temporary view of the housing_data dataframe \n",
    "\n",
    "housing_data.createOrReplaceTempView(\"dc_properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af5bfdf4-47fe-4954-9652-cc3e1f02a655",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from dc_properties "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "086eb26c-7b4b-4d29-9f91-4c98db2b0a25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT AVG(PRICE) AS average_price FROM dc_properties;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b534a9b6-5855-4dc8-9cec-c01f62f67e87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT QUADRANT, AVG(PRICE) AS average_price\n",
    "FROM dc_properties\n",
    "WHERE PRICE IS NOT NULL AND QUADRANT IS NOT NULL\n",
    "GROUP BY QUADRANT;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5c8d93d-d9f4-4acf-92e4-70140e58023a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT QUADRANT, STRUCT, AVG(PRICE) AS average_price\n",
    "FROM dc_properties\n",
    "WHERE PRICE IS NOT NULL AND QUADRANT IS NOT NULL AND STRUCT IS NOT NULL\n",
    "GROUP BY QUADRANT, STRUCT;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95fa5357-03b0-4cbc-bca9-83f12326c660",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9be6893-3e6a-45a5-8778-3ac0f6137b6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# convert spark data frame to a pandas data frame to create visualizations\n",
    "housing_data_pd = housing_data.toPandas()\n",
    "\n",
    "#display pandas data\n",
    "housing_data_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0263b283-4fee-4334-8fe2-321fd4427ebb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Correlation matrix\n",
    "housing_data_pd.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "961ebf40-317a-4b3e-8ddb-310c3301346f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Correlation Heat Map\n",
    "import plotly.express as px\n",
    "\n",
    "fig_cm = px.imshow(\n",
    "    housing_data_pd.corr().round(2),\n",
    "    color_continuous_scale= 'sunset',\n",
    "    text_auto=True,\n",
    "    template= 'ggplot2',\n",
    "    title= \"House Prediction Correlation Heatmap\"\n",
    ")\n",
    "\n",
    "# Make the figure bigger\n",
    "fig_cm.update_layout(\n",
    "    height=800,  # set the height in pixels\n",
    "    width=800    # set the width in pixels\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig_cm.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbe4280f-150b-40d2-b50d-b8c99b6dcb36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualize which Quadrant in DC had the most home buyers\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(15, 7))\n",
    "sns.countplot(data=housing_data_pd, x='QUADRANT', order=housing_data_pd['QUADRANT'].value_counts(ascending=False).index)\n",
    "\n",
    "plt.xlabel( \"Quadrant\" , size = 12, weight='bold') # Set label for x-axis \n",
    "\n",
    "plt.ylabel( \"Count\" , size = 12, weight='bold') # Set label for y-axis \n",
    "  \n",
    "plt.title( \"Homebuyer Count by Quadrant\" , size = 14, weight='bold') # Set title for plot \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27133d86-4d83-4113-8311-83537c3d9a3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualize which Ward in DC had the most home buyers\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(15, 7))\n",
    "sns.countplot(data=housing_data_pd, x='WARD', order=housing_data_pd['WARD'].value_counts(ascending=False).index)\n",
    "\n",
    "plt.xlabel( \"Ward\" , size = 12, weight='bold') # Set label for x-axis \n",
    "\n",
    "plt.ylabel( \"Count\" , size = 12, weight='bold') # Set label for y-axis \n",
    "  \n",
    "plt.title( \"Homebuyer Count by Ward\" , size = 14, weight='bold') # Set title for plot \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9949a8bd-f3bb-4702-904e-ac16cb0a1059",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualize which Structure in DC had the most home buyers\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(15, 7))\n",
    "sns.countplot(data=housing_data_pd, x='STRUCT', order=housing_data_pd['STRUCT'].value_counts(ascending=False).index)\n",
    "\n",
    "plt.xlabel( \"Structure\" , size = 12, weight='bold') # Set label for x-axis \n",
    "\n",
    "plt.ylabel( \"Count\" , size = 12, weight='bold') # Set label for y-axis \n",
    "  \n",
    "plt.title( \"Homebuyer Count by Structure\" , size = 14, weight='bold') # Set title for plot \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1215c61-bb8a-445e-b85f-1ba6ce8c0d54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Time Series Analysis - Sale Date on the Average of the Prices\n",
    "import pandas as pd\n",
    "\n",
    "#Converting the 'SALEDATE' column to datetime format\n",
    "housing_data_pd['SALEDATE'] = pd.to_datetime(housing_data_pd['SALEDATE'])\n",
    "\n",
    "# 'PRICE' column will be cleaned and it will remove non-numeric characters or other data types\n",
    "housing_data_pd['PRICE'] = pd.to_numeric(housing_data_pd['PRICE'], errors='coerce')\n",
    "\n",
    "# Dropping the rows with no values for 'PRICE'\n",
    "housing_data_pd.dropna(subset=['PRICE'], inplace=True)\n",
    "\n",
    "# Data is assorted in ascending order\n",
    "housing_data_pd.sort_values('SALEDATE', inplace=True)\n",
    "\n",
    "# Sales by the date will be grouped by the average of the price\n",
    "sales_date = housing_data_pd.groupby('SALEDATE')['PRICE'].mean().reset_index()\n",
    "\n",
    "# Plotting the timeseries of the overall average prices based on the date and time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(sales_date['SALEDATE'], sales_date['PRICE'], marker='o', linestyle='-')\n",
    "plt.title('Average Home Prices Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average Price')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91b957b8-1f9d-459e-8939-f782f501b23f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# House Prices by Property Grade\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "sns.boxplot(x='GRADE', y='PRICE', data=housing_data_pd)\n",
    "plt.title('House Prices by Property Grade')\n",
    "plt.xlabel('Grade')\n",
    "plt.ylabel('Price')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "072ab3c9-e059-4c44-867e-d70c06836b00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Houses Prices by Property Structure\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "sns.boxplot(x='STRUCT', y='PRICE', data=housing_data_pd)\n",
    "plt.title('House Prices by Property Structure')\n",
    "plt.xlabel('Structure')\n",
    "plt.ylabel('Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3360209-e316-4f10-83d1-36c077cd0159",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# House Prices by Living Area\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(housing_data_pd['LIVING_GBA'], housing_data_pd['PRICE'], alpha=0.5)\n",
    "plt.title('House Prices vs. Living Area')\n",
    "plt.xlabel('Living Area')\n",
    "plt.ylabel('Price')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d1f6bfb-1d03-46fa-a248-b49654ceb802",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualize which the type of house features have the highest value count\n",
    "\n",
    "# categorical columns chosen to portray the type of house features that has the highest value of count\n",
    "categorical_columns = ['STYLE', 'STRUCT', 'GRADE', 'CNDTN', 'EXTWALL', 'ROOF', 'INTWALL']\n",
    "\n",
    "# Filtering\n",
    "categorical_data = housing_data_pd[categorical_columns]\n",
    "\n",
    "# Subplots\n",
    "num_plots = len(categorical_columns)\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, column in enumerate(categorical_data):\n",
    "    plt.subplot((num_plots // 2) + 1, 2, i + 1)\n",
    "    sns.countplot(data=categorical_data, x=column, order=categorical_data[column].value_counts().index)\n",
    "    plt.title(f'Count of {column}')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8c6e718-0fe6-4c1e-9ade-27c5ac6f1ff2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove the outliers in the variables: ROOMS, BATHRM, HF_BATHRM, and BEDRM \n",
    "housing_data = housing_data.filter((col(\"ROOMS\") < 100) & (col(\"ROOMS\") >= col(\"BEDRM\")) & (col(\"BATHRM\") < 24))\n",
    "\n",
    "display(housing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8004db1e-e1c6-42bf-bc9b-c94a9ebded33",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Feature Preprocessing/Training & Test Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ea7d36b-70fa-4005-9143-36657aa75d14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Imputer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Define categorical columns\n",
    "categorical_cols = ['HEAT', 'AC', 'QUALIFIED', 'STYLE', 'STRUCT', 'GRADE', 'CNDTN', 'EXTWALL', 'ROOF', 'INTWALL', 'SOURCE', 'WARD', 'QUADRANT']\n",
    "\n",
    "# Create a list of StringIndexers for categorical columns\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + '_index', handleInvalid='keep') for col in categorical_cols]\n",
    "\n",
    "# Create a list of OneHotEncoders for indexed categorical columns\n",
    "encoders = [OneHotEncoder(inputCol=col + '_index', outputCol=col + '_encoded') for col in categorical_cols]\n",
    "\n",
    "# Define the numeric columns\n",
    "numeric_cols = ['BATHRM', 'HF_BATHRM', 'NUM_UNITS', 'ROOMS', 'BEDRM', 'AYB', 'YR_RMDL', 'EYB', 'STORIES',\n",
    "                'GBA', 'BLDG_NUM', 'KITCHENS', 'FIREPLACES', 'LANDAREA', 'LIVING_GBA']\n",
    "\n",
    "# Impute missing values in numeric columns using the mean\n",
    "imputer = Imputer(inputCols=numeric_cols, outputCols=[col + \"_imputed\" for col in numeric_cols], strategy=\"mean\")\n",
    "\n",
    "# Assemble all features into a single vector\n",
    "assembler = VectorAssembler(inputCols=[col + \"_imputed\" for col in numeric_cols] + [col + '_encoded' for col in categorical_cols],\n",
    "                            outputCol='features')\n",
    "\n",
    "# Define the label column\n",
    "label = 'PRICE'\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders + [imputer, assembler])\n",
    "\n",
    "# Fit the pipeline to the data\n",
    "pipeline_model = pipeline.fit(housing_data)\n",
    "\n",
    "# Transform the data\n",
    "transformed_data = pipeline_model.transform(housing_data)\n",
    "\n",
    "# Show the transformed data\n",
    "transformed_data.select('features', label).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f284793d-185e-43ed-809c-c515cab17e37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display the first few rows of the transformed dataset\n",
    "display(transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c65bca5-c42a-48d5-ad40-f7df89ee56b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check for NULL Values after feature processing\n",
    "null_counts = transformed_data.select([count(when(col(c).isNull(), c)).alias(c) for c in transformed_data.columns])\n",
    "\n",
    "# Display the null counts row\n",
    "display(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c5eb9e6-ab0b-43cd-8bfe-8dfe3816df18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create training and test data sets\n",
    "train_df, test_df = transformed_data.randomSplit([0.8, 0.2], seed=42)\n",
    "print(train_df.cache().count())\n",
    "print(test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43c4b27a-7333-4d91-a949-aac20ef227cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display training data\n",
    "display(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d358961-33dd-4d90-9cc3-f385647fc71c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display test data\n",
    "display(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c81e1cb-2e79-4937-8858-4acdd5e4a543",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Modeling and Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3708bd56-274f-4bf6-ac51-219262392754",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Random Forest Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06ea5372-025b-4f83-b033-c30027b2069d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create Random Forest object for regression\n",
    "rf_regressor = RandomForestRegressor(featuresCol='features', labelCol='PRICE', numTrees=10, maxDepth=5, seed=42)\n",
    "\n",
    "# Fit the Random Forest model to the training data\n",
    "rf_model = rf_regressor.fit(train_df)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_rf = rf_model.transform(test_df)\n",
    "\n",
    "# Evaluate the model using a regression evaluator - RMSE\n",
    "evaluator = RegressionEvaluator(labelCol='PRICE', predictionCol='prediction', metricName='rmse')\n",
    "rmse = evaluator.evaluate(predictions_rf)\n",
    "print(\"Root Mean Squared Error:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83a47031-e297-41f5-b8c7-4a08443ea6da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the RMSE, R-Squared and Mean Squared Error for the Random Forest Regressor Model\n",
    "\n",
    "# Evaluate the model - RMSE\n",
    "rf_evaluator_rmse = RegressionEvaluator(labelCol='PRICE', predictionCol='prediction', metricName='rmse')\n",
    "rf_rmse = rf_evaluator_rmse.evaluate(predictions_rf)\n",
    "print(\"Root Mean Squared Error:\", rf_rmse)\n",
    "\n",
    "# Evaluate the model - R-Sqaured\n",
    "rf_evaluator_r2 = RegressionEvaluator(labelCol='PRICE', predictionCol='prediction', metricName='r2')\n",
    "rf_r2 = rf_evaluator_r2.evaluate(predictions_rf)\n",
    "\n",
    "print(\"R-squared:\", rf_r2)\n",
    "\n",
    "# Evaluate the model - Mean Squared Error\n",
    "rf_mse = evaluator.evaluate(predictions_rf, {evaluator.metricName: 'mse'})\n",
    "print(\"Mean Squared Error:\", rf_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "567d68c8-bf59-480d-a1fd-2af620335c56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show the predictions for the Random Forest model\n",
    "predictions_rf.select('features', 'PRICE', 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31d1e5a5-ed46-4925-a869-c0d67e6f0639",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Determine Feature Importance for the Random Forest Model\n",
    "importances_rf = rf_model.featureImportances.toArray()\n",
    "\n",
    "# Get feature names from the vector assembler\n",
    "feature_names = assembler.getInputCols()\n",
    "\n",
    "# Create a Pandas DataFrame with feature names and importance scores\n",
    "importances_df_pd_rf = pd.DataFrame(list(zip(feature_names, importances_rf)), columns=[\"Attribute_rf\", \"Importance_rf\"])\n",
    "\n",
    "# Create a Spark DataFrame from the Pandas DataFrame\n",
    "importances_df = spark.createDataFrame(importances_df_pd_rf)\n",
    "\n",
    "# Sort the DataFrame by importance in descending order\n",
    "importances_df = importances_df.sort(\"Importance_rf\", ascending=False)\n",
    "\n",
    "# Show the result\n",
    "importances_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f4b9c24-0302-4cab-b47b-8b77783df8d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visalizing the feature importance for Random Forest using a bar plot.\n",
    "importances_df_pd = importances_df.toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(importances_df_pd['Attribute_rf'], importances_df_pd['Importance_rf'], color='blue')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.xticks(rotation=45, ha='right') \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "908e70c5-2dcb-4dd9-bb18-9d3078a9b264",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Linear Regression Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25a8e382-4c91-4db5-822f-da3e1fa0fd95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rename the 'PRICE' column to 'label' in both the training and test data sets to run Linear Regression in Pyspark\n",
    "\n",
    "train_data = train_df.withColumnRenamed('PRICE', 'label')\n",
    "test_data = test_df.withColumnRenamed('PRICE', 'label')\n",
    "\n",
    "# Display training data\n",
    "display(train_data)\n",
    "\n",
    "# Display test data\n",
    "display(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7449db92-69d6-4078-b73a-8b0d5b252f15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Linear Regression using Pyspark\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Create a Linear Regression object\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Fit the model to the training data\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cf5c139-6bf2-4685-8dac-7e96efd36248",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the RMSE, R-Squared and Mean Squared Error for the Linear Regression Model\n",
    "\n",
    "# Evaluate the model - RMSE\n",
    "reg_evaluator_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "regression_rmse = reg_evaluator_rmse.evaluate(predictions)\n",
    "\n",
    "print(f\"Root Mean Squared Error (RMSE): {regression_rmse}\")\n",
    "\n",
    "# Evaluate the model - R-Squared\n",
    "reg_evaluator_r2 = RegressionEvaluator(labelCol='label', predictionCol='prediction', metricName='r2')\n",
    "regression_r2 = reg_evaluator_r2.evaluate(predictions)\n",
    "\n",
    "print(\"R-squared:\", regression_r2)\n",
    "\n",
    "# Evaluate the model - Mean Squared Error\n",
    "reg_evaluator_mse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "regression_mse = reg_evaluator_mse.evaluate(predictions)\n",
    "print(f\"Mean Squared Error (MSE): {regression_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64851f02-f37e-4665-8381-9b5e658f2058",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show the predictions for the Linear Regression model\n",
    "predictions.select('features', 'label', 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f94ac299-4d12-413d-9267-01fe4647dcb3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Determine the feature importance for the Linear Regression Model (pyspark)\n",
    "\n",
    "# Get coefficients from the model\n",
    "coefficients = lr_model.coefficients.toArray()\n",
    "\n",
    "# Get feature names from the vector assembler\n",
    "feature_names_lr = assembler.getInputCols()\n",
    "\n",
    "# Create a Pandas DataFrame with feature names and coefficients\n",
    "importances_df_pd_lr = pd.DataFrame(list(zip(feature_names_lr, coefficients)), columns=[\"Attribute_lr\", \"Coefficient_lr\"])\n",
    "\n",
    "# Sort the DataFrame by absolute coefficient values in descending order\n",
    "importances_df_pd_lr = importances_df_pd_lr.assign(Absolute_Coefficient_lr=lambda x: abs(x['Coefficient_lr']))\n",
    "importances_df_pd_lr = importances_df_pd_lr.sort_values(by='Absolute_Coefficient_lr', ascending=False)\n",
    "\n",
    "print(importances_df_pd_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2f3d2f1-256b-4dde-97e7-0a63741a60f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Visalizing the feature importance for Liinear Regression using a bar plot.\n",
    "importances_df_pd_lr = importances_df_pd_lr.sort_values(by='Absolute_Coefficient_lr', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(importances_df_pd_lr['Attribute_lr'], importances_df_pd_lr['Absolute_Coefficient_lr'], color='green')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Absolute Coefficient')\n",
    "plt.title('Linear Regression Feature Importance')\n",
    "plt.xticks(rotation=45, ha='right') \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dab05803-60ed-43cc-bdd8-76b0860b3ac7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Here is a simple linear regression model using sklearn that prints the accuracy and r^2 score in comparison to the other one. It looks like that the linear regression algorithm is not a good fit to this dataset. \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 'PRICE' is the target variable\n",
    "#Selected few categorical features based on the dataset\n",
    "selected_features = ['BATHRM', 'HF_BATHRM', 'ROOMS', 'BEDRM', 'AYB', 'YR_RMDL', 'EYB', 'STORIES', 'GBA', 'FIREPLACES', 'LANDAREA']\n",
    "\n",
    "# Filtering the PRICE column \n",
    "data_for_modeling = housing_data_pd[selected_features + ['PRICE']].copy()\n",
    "\n",
    "# Dropped the rows with missing values\n",
    "data_for_modeling.dropna(inplace=True)\n",
    "\n",
    "# Split the data into training and testing sets (80%/20%)\n",
    "X = data_for_modeling[selected_features]\n",
    "y = data_for_modeling['PRICE']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# fitting the training set into the Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "#Accuracy\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f\"Model Accuracy: {accuracy}\")\n",
    "\n",
    "# Model evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"R-squared (R2): {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaa26c28-59f3-4b03-bafd-61ff92aa5d2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Predictions for the model\n",
    "display(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "903dd8b1-c14b-40dd-b6f4-b93bc693c76d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Decision Tree Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a79adee7-df77-4522-af7e-42428c910d8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create a Decision Tree Regressor\n",
    "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Fit the model to the training data\n",
    "dt_model = dt.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = dt_model.transform(test_data)\n",
    "\n",
    "# Evaluate RMSE\n",
    "reg_evaluator_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "regression_rmse = reg_evaluator_rmse.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE): {regression_rmse}\")\n",
    "\n",
    "# Evaluate R-Squared\n",
    "reg_evaluator_r2 = RegressionEvaluator(labelCol='label', predictionCol='prediction', metricName='r2')\n",
    "regression_r2 = reg_evaluator_r2.evaluate(predictions)\n",
    "print(\"R-squared:\", regression_r2)\n",
    "\n",
    "# Evaluate Mean Squared Error\n",
    "reg_evaluator_mse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "regression_mse = reg_evaluator_mse.evaluate(predictions)\n",
    "print(f\"Mean Squared Error (MSE): {regression_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "032b6139-2020-4d21-881a-ba6b482493a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show the predictions for the Decision Tree model\n",
    "predictions.select('features', 'label', 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d212469-1f31-4bde-a20b-1999909a6b53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get feature importances from the model\n",
    "feature_importances = dt_model.featureImportances.toArray()\n",
    "\n",
    "# Get feature names from the vector assembler\n",
    "feature_names_dt = assembler.getInputCols()\n",
    "\n",
    "# Create a Pandas DataFrame with feature names and importances\n",
    "importances_df_pd_dt = pd.DataFrame(list(zip(feature_names_dt, feature_importances)), columns=[\"Attribute_dt\", \"Importance_dt\"])\n",
    "\n",
    "# Sort the DataFrame by importance values in descending order\n",
    "importances_df_pd_dt = importances_df_pd_dt.sort_values(by='Importance_dt', ascending=False)\n",
    "\n",
    "print(importances_df_pd_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "568caed9-889f-4ae5-bd3f-bf32f5244907",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get feature importances from the model\n",
    "feature_importances = dt_model.featureImportances.toArray()\n",
    "\n",
    "# Get feature names from the vector assembler\n",
    "feature_names_dt = assembler.getInputCols()\n",
    "\n",
    "# Create a Pandas DataFrame with feature names and importances\n",
    "importances_df_pd_dt = pd.DataFrame(list(zip(feature_names_dt, feature_importances)), columns=[\"Attribute\", \"Importance\"])\n",
    "\n",
    "# Sort the DataFrame by importance values in descending order\n",
    "top_importances_df_pd_dt = importances_df_pd_dt.sort_values(by='Importance', ascending=False).head(10)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_importances_df_pd_dt['Attribute'], top_importances_df_pd_dt['Importance'], color='blue')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importances from Decision Tree')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffb55eab-26af-4531-8f78-e8f0a16be5a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the decision tree model\n",
    "tree_model = dt_model\n",
    "\n",
    "# Print out the decision tree\n",
    "print(tree_model.toDebugString)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1105627068972211,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "AIT614-SecDL2_Team5_House Pricing Analysis_Notebook 1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
